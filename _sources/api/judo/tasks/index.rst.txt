judo.tasks
==========

.. py:module:: judo.tasks


Submodules
----------

.. toctree::
   :maxdepth: 1

   /api/judo/tasks/base/index
   /api/judo/tasks/caltech_leap_cube/index
   /api/judo/tasks/cartpole/index
   /api/judo/tasks/cost_functions/index
   /api/judo/tasks/cylinder_push/index
   /api/judo/tasks/fr3_pick/index
   /api/judo/tasks/leap_cube/index
   /api/judo/tasks/leap_cube_down/index






Package Contents
----------------

.. py:class:: Task(model_path: pathlib.Path | str = '', sim_model_path: pathlib.Path | str | None = None)

   Bases: :py:obj:`abc.ABC`, :py:obj:`Generic`\ [\ :py:obj:`ConfigT`\ ]


   
   Task definition.

   .. py:attribute:: spec


   .. py:attribute:: model


   .. py:attribute:: data


   .. py:attribute:: model_path
      :value: ''



   .. py:attribute:: sim_model


   .. py:property:: time
      :type: float


      
      Returns the current simulation time.


   .. py:method:: reward(states: numpy.ndarray, sensors: numpy.ndarray, controls: numpy.ndarray, config: ConfigT, system_metadata: dict[str, Any] | None = None) -> numpy.ndarray
      :abstractmethod:


      
      Abstract reward function for task.

      :param states: The rolled out states (after the initial condition). Shape=(num_rollouts, T, nq + nv).
      :param sensors: The rolled out sensors readings. Shape=(num_rollouts, T, total_num_sensor_dims).
      :param controls: The rolled out controls. Shape=(num_rollouts, T, nu).
      :param config: The current task config (passed in from the top-level controller).
      :param system_metadata: Any additional metadata from the system that is useful for computing the reward. For
                              example, in the cube rotation task, the system could pass in new goal cube orientations to the
                              controller here.

      :returns: *rewards* -- The reward for each rollout. Shape=(num_rollouts,).


   .. py:property:: nu
      :type: int


      
      Number of control inputs. The same as the MjModel for this task.


   .. py:property:: actuator_ctrlrange
      :type: numpy.ndarray


      
      Mujoco actuator limits for this task.


   .. py:method:: reset() -> None

      
      Reset behavior for task. Sets config + velocities to zeros.


   .. py:property:: dt
      :type: float


      
      Returns Mujoco physics timestep for default physics task.


   .. py:method:: pre_rollout(curr_state: numpy.ndarray, config: ConfigT) -> None

      
      Pre-rollout behavior for task (does nothing by default).

      :param curr_state: Current state of the task. Shape=(nq + nv,).
      :param config: The current task config (passed in from the top-level controller).


   .. py:method:: post_rollout(states: numpy.ndarray, sensors: numpy.ndarray, controls: numpy.ndarray, config: ConfigT, system_metadata: dict[str, Any] | None = None) -> None

      
      Post-rollout behavior for task (does nothing by default).

      Same inputs as in reward function.


   .. py:method:: pre_sim_step() -> None

      
      Pre-simulation step behavior for task (does nothing by default).


   .. py:method:: post_sim_step() -> None

      
      Post-simulation step behavior for task (does nothing by default).


   .. py:method:: get_sim_metadata() -> dict[str, Any]

      
      Returns metadata from the simulation.

      We need this function because the simulation thread runs separately from the controller thread, but there are
      task objects in both. This function is used to pass information about the simulation's version of the task to
      the controller's version of the task.

      For example, the LeapCube task has a goal quaternion that is updated in the simulation thread based on whether
      the goal was reached (which the controller thread doesn't know about). When a new goal is set, it must be passed
      to the controller thread via this function.


   .. py:method:: optimizer_warm_start() -> numpy.ndarray

      
      Returns a warm start for the optimizer.

      This is used to provide an initial guess for the optimizer when optimizing the task before any iterations.


.. py:class:: TaskConfig

   
   Base task configuration dataclass.

.. py:class:: CaltechLeapCube(model_path: str = XML_PATH, sim_model_path: str = SIM_XML_PATH)

   Bases: :py:obj:`judo.tasks.leap_cube.LeapCube`


   
   Defines the LEAP cube rotation task.

   .. py:attribute:: goal_pos


   .. py:attribute:: goal_quat


   .. py:attribute:: qpos_home


   .. py:attribute:: reset_command


.. py:class:: CaltechLeapCubeConfig

   Bases: :py:obj:`judo.tasks.leap_cube.LeapCubeConfig`


   
   Reward configuration LEAP cube rotation task.

.. py:class:: Cartpole(model_path: str = XML_PATH, sim_model_path: str | None = None)

   Bases: :py:obj:`judo.tasks.base.Task`\ [\ :py:obj:`CartpoleConfig`\ ]


   
   Defines the cartpole balancing task.

   .. py:method:: reward(states: numpy.ndarray, sensors: numpy.ndarray, controls: numpy.ndarray, config: CartpoleConfig, system_metadata: dict[str, Any] | None = None) -> numpy.ndarray

      
      Implements the cartpole reward from MJPC.

      Maps a list of states, list of controls, to a batch of rewards (summed over time) for each rollout.

      The cartpole reward has four terms:

      ::

         * `vertical_rew`, penalizing the distance between the pole angle and vertical.
         * `centered_rew`, penalizing the distance from the cart to the origin.
         * `velocity_rew` penalizing squared linear and angular velocity.
         * `control_rew` penalizing any actuation.


      Since we return rewards, each penalty term is returned as negative. The max reward is zero.

      :returns: A list of rewards shaped (batch_size,) where reward at index i represents the reward for that batched traj


   .. py:method:: reset() -> None

      
      Resets the model to a default (random) state.


.. py:class:: CartpoleConfig

   Bases: :py:obj:`judo.tasks.base.TaskConfig`


   
   Reward configuration for the cartpole task.

   .. py:attribute:: w_vertical
      :type:  float
      :value: 10.0



   .. py:attribute:: w_centered
      :type:  float
      :value: 10.0



   .. py:attribute:: w_velocity
      :type:  float
      :value: 0.1



   .. py:attribute:: w_control
      :type:  float
      :value: 0.1



   .. py:attribute:: p_vertical
      :type:  float
      :value: 0.01



   .. py:attribute:: p_centered
      :type:  float
      :value: 0.1



.. py:class:: CylinderPush(model_path: str = XML_PATH, sim_model_path: str | None = None)

   Bases: :py:obj:`judo.tasks.base.Task`\ [\ :py:obj:`CylinderPushConfig`\ ]


   
   Defines the cylinder push balancing task.

   .. py:method:: reward(states: numpy.ndarray, sensors: numpy.ndarray, controls: numpy.ndarray, config: CylinderPushConfig, system_metadata: dict[str, Any] | None = None) -> numpy.ndarray

      
      Implements the cylinder push reward from MJPC.

      Maps a list of states, list of controls, to a batch of rewards (summed over time) for each rollout.

      The cylinder push reward has four terms:

      ::

         * `pusher_reward`, penalizing the distance between the pusher and the cart.
         * `velocity_reward` penalizing squared linear velocity of the pusher.
         * `goal_reward`, penalizing the distance from the cart to the goal.


      Since we return rewards, each penalty term is returned as negative. The max reward is zero.


   .. py:method:: reset() -> None

      
      Resets the model to a default (random) state.


.. py:class:: CylinderPushConfig

   Bases: :py:obj:`judo.tasks.base.TaskConfig`


   
   Reward configuration for the cylinder push task.

   .. py:attribute:: w_pusher_proximity
      :type:  float
      :value: 0.5



   .. py:attribute:: w_pusher_velocity
      :type:  float
      :value: 0.0



   .. py:attribute:: w_cart_position
      :type:  float
      :value: 0.1



   .. py:attribute:: pusher_goal_offset
      :type:  float
      :value: 0.25



   .. py:attribute:: goal_pos
      :type:  numpy.ndarray


.. py:class:: FR3Pick(model_path: str = XML_PATH, sim_model_path: str | None = None)

   Bases: :py:obj:`judo.tasks.base.Task`\ [\ :py:obj:`FR3PickConfig`\ ]


   
   Defines the FR3 pick task.

   .. py:attribute:: reset_command


   .. py:attribute:: obj_pos_adr


   .. py:attribute:: obj_pos_slice


   .. py:attribute:: obj_vel_slice


   .. py:attribute:: obj_angvel_slice


   .. py:attribute:: arm_pos_slice


   .. py:attribute:: left_finger_obj_sensor


   .. py:attribute:: left_finger_obj_adr


   .. py:attribute:: right_finger_obj_sensor


   .. py:attribute:: right_finger_obj_adr


   .. py:attribute:: left_finger_table_sensor


   .. py:attribute:: left_finger_table_adr


   .. py:attribute:: right_finger_table_sensor


   .. py:attribute:: right_finger_table_adr


   .. py:attribute:: grasp_site_sensor


   .. py:attribute:: grasp_site_adr


   .. py:attribute:: obj_table_sensor


   .. py:attribute:: obj_table_adr


   .. py:attribute:: ee_z_sensor


   .. py:attribute:: ee_z_adr


   .. py:attribute:: ee_z_slice


   .. py:attribute:: phase


   .. py:method:: in_goal_xy(curr_state: numpy.ndarray, config: FR3PickConfig) -> numpy.ndarray

      
      Checks if the object is somewhere in the tube above the goal position of radius r.

      :param curr_state: The current state value. Shape=(nq + nv,).
      :param config: The task configuration.

      :returns: *in_goal* -- A bool indicating whether the object is in the goal region. Shape=(,).


   .. py:method:: check_sensor_dists(sensors: numpy.ndarray, pair: Literal['left_finger_obj', 'right_finger_obj', 'left_finger_table', 'right_finger_table', 'obj_table']) -> numpy.ndarray

      
      Computes the distance between a specified pair of bodies.

      :param sensors: The sensor values. Shape=(num_rollouts, T, total_sensor_dim).
      :param pair: The pair of bodies to check contact for. One of "left_finger_obj", "right_finger_obj", or "obj_table".

      :returns: *dist* -- An array indicating the distance between the specified pair. Shape=(num_rollouts, T).


   .. py:method:: pre_rollout(curr_state: numpy.ndarray, config: FR3PickConfig) -> None

      
      Computes the current phase of the system.


   .. py:method:: reward(states: numpy.ndarray, sensors: numpy.ndarray, controls: numpy.ndarray, config: FR3PickConfig, system_metadata: dict[str, Any] | None = None) -> numpy.ndarray

      
      Implements the LEAP cube rotation tracking task reward.

      The reward function switches between 4 modes:


      * LIFT: The object is lifted from the table.
      * MOVE: The object is moved to the goal position.
      * PLACE: The object is placed on the table.
      * HOMING: The robot arm is returned to the home position.

      There are also global rewards that are always applied:


      * Upright: The end-effector is upright.
      * Collision: The robot hand is not touching the table.
      * Qvel: The robot arm is not moving too fast.


   .. py:method:: reset() -> None

      
      Resets the model to a default state with random goal.


.. py:class:: FR3PickConfig

   Bases: :py:obj:`judo.tasks.base.TaskConfig`


   
   Reward configuration for FR3 pick task.

   .. py:attribute:: lift_weights
      :type:  LiftConfig


   .. py:attribute:: move_weights
      :type:  MoveConfig


   .. py:attribute:: place_weights
      :type:  PlaceConfig


   .. py:attribute:: global_weights
      :type:  GlobalConfig


   .. py:attribute:: goal_pos
      :type:  numpy.ndarray


   .. py:attribute:: goal_radius
      :type:  float
      :value: 0.05



   .. py:attribute:: pick_height
      :type:  float
      :value: 0.3



.. py:class:: LeapCube(model_path: str = XML_PATH, sim_model_path: str | None = SIM_XML_PATH)

   Bases: :py:obj:`judo.tasks.base.Task`\ [\ :py:obj:`LeapCubeConfig`\ ]


   
   Defines the LEAP cube rotation task.

   .. py:attribute:: goal_pos


   .. py:attribute:: goal_quat


   .. py:attribute:: qpos_home


   .. py:attribute:: reset_command


   .. py:method:: reward(states: numpy.ndarray, sensors: numpy.ndarray, controls: numpy.ndarray, config: LeapCubeConfig, system_metadata: dict[str, Any] | None = None) -> numpy.ndarray

      
      Implements the LEAP cube rotation tracking task reward.


   .. py:method:: post_sim_step() -> None

      
      Checks if the cube has dropped and resets if so.


   .. py:method:: reset() -> None

      
      Resets the model to a default state with random goal.


   .. py:method:: get_sim_metadata() -> dict[str, Any]

      
      Returns the simulation's goal quat.


.. py:class:: LeapCubeConfig

   Bases: :py:obj:`judo.tasks.base.TaskConfig`


   
   Reward configuration LEAP cube rotation task.

   .. py:attribute:: w_pos
      :type:  float
      :value: 100.0



   .. py:attribute:: w_rot
      :type:  float
      :value: 0.1



.. py:class:: LeapCubeDown(model_path: str = XML_PATH, sim_model_path: str = SIM_XML_PATH)

   Bases: :py:obj:`judo.tasks.leap_cube.LeapCube`


   
   Defines the LEAP cube with palm down rotation task.

   .. py:attribute:: goal_pos


   .. py:attribute:: goal_quat


   .. py:attribute:: qpos_home


   .. py:attribute:: reset_command


.. py:class:: LeapCubeDownConfig

   Bases: :py:obj:`judo.tasks.leap_cube.LeapCubeConfig`


   
   Reward configuration LEAP cube rotation task.

   .. py:attribute:: w_rot
      :type:  float
      :value: 0.05



.. py:function:: get_registered_tasks() -> Dict[str, Tuple[Type[base.Task], Type[base.TaskConfig]]]

   
   Returns a dictionary of registered tasks.

.. py:function:: register_task(name: str, task_type: Type[base.Task], task_config_type: Type[base.TaskConfig]) -> None

   
   Registers a new task.

